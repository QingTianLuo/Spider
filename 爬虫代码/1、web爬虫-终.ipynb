{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab62dcf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys    \n",
    "import re    \n",
    "import requests    \n",
    "from bs4 import  BeautifulSoup    \n",
    "import traceback    \n",
    "import pandas as pd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0d1e0146",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers={  \n",
    "'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36 Core/1.77.119.400 QQBrowser/10.9.4817.400'  \n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4a0677a",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_PREF = 'http://172.23.254.2:40000' # 用于内容页完整URL的生成   \n",
    "URL = 'http://172.23.254.2:40000/wuyou/{}' # {}内放列表页的页码 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7694318e",
   "metadata": {},
   "source": [
    "爬取网页内容，并存储到指定文件夹，该文件夹下每个txt文件表示所要爬取的每个网页"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cae7bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_html(page_num):    \n",
    "    \"\"\"    \n",
    "    获取网页html源码    \n",
    "    @ page_num 指定要爬网页的页码    \n",
    "    \"\"\"   \n",
    "    real_url = URL.format(page_num)    \n",
    "    res = requests.get(real_url, headers=headers)    # 设置headers，是为了模拟成浏览器去访问这些网站  \n",
    "    #res = requests.get(real_url)  \n",
    "    res.encoding = 'utf-8'                           # 设置编码格式，保证内容正常显示  \n",
    "    html_source = res.text                           # 获取文本一般使用res.text  \n",
    "    status_code = res.status_code                    # 状态码，200（成功）表示服务器已成功处理了请求  \n",
    "    return html_source,status_code                   # 返回网页源码，状态码  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "581589f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_html(keyword, total_page_number):    \n",
    "    \"\"\"    \n",
    "    利用BeautifulSoup解析页面，获取每一个职位的内容页URL    \n",
    "    所有的查询结果保存到一个list中返回。    \n",
    "    @ keyword 职位中包含的关键词  \n",
    "    @ total_page_number 查询列表页的总页数  \n",
    "    \"\"\"    \n",
    "    ret_list = []                                                              # 存放职位链接  \n",
    "    # try-except，可以保证程序在遇到问题时先继续运行下去  \n",
    "    try:    \n",
    "        for page in range(1,total_page_number+1):                             # 遍历前total_page_number个列表页  \n",
    "            print (\"parsing page {} of {}\".format(page, total_page_number))    # 打印列表页爬取的进度  \n",
    "            html_source,status_code = get_html(page)    \n",
    "            if status_code == 200:                                            # 判断是否爬取成功  \n",
    "                soup = BeautifulSoup(html_source, 'html.parser')              # 指定Beautiful的解析器为“html.parser”  \n",
    "                t1 = soup.select('.t1 span a')                                # 在分析网页源代码的基础上，通过类名和标签进行查找  \n",
    "                for i in range(1,len(t1)):    \n",
    "                    job = t1[i].get('title')                                  # 获取职位    \n",
    "                    if keyword not in job:                                   # 判断关键词是否存在于职位名称中  \n",
    "                        continue                                             # 职位名称不符合要求，跳过当前循环  \n",
    "                    href = t1[i].get('href')                                  # 获取链接    \n",
    "                    ret_list.append(href)                                     # 将符合要求的内容页链接存入列表中  \n",
    "        return ret_list                                                      # 返回存放职位链接的列表  \n",
    "    except TimeoutError:                                                     # 连接尝试失败  \n",
    "        print(\"请求失败\")    \n",
    "        return  None  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd930363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_html(save_path,ret_list):    \n",
    "    \"\"\"    \n",
    "    利用解析到的职位的url获取职位详情页    \n",
    "    \"\"\"    \n",
    "    cnt = 0    \n",
    "    for item in ret_list:                                               # 遍历所有的职位链接  \n",
    "        file_name = item.split(\"/\")[-1]+\".txt\"                          # 以URL地址最后的数字来命名txt文件   \n",
    "        fr = open(os.path.join(save_path,file_name),\"w\")                # 'w'保证生成的txt文件可以写入内容  \n",
    "        cnt += 1    \n",
    "        print (\"geting more info for %d of %d\"%(cnt, len(ret_list)))    # 打印目前获取职位详情的进度  \n",
    "        try:  \n",
    "            href = URL_PREF+item                                        # 根据内容页的URL规律，得到内容页完整的URL地址  \n",
    "            res = requests.get(href)    \n",
    "            res.encoding = 'utf-8'    \n",
    "            html_source = res.text    \n",
    "            fr.write(html_source)                                       # 将爬取的网页源代码写入到对应的txt文件中  \n",
    "            fr.close()                                                  # 关闭打开的txt文件  \n",
    "        except:    \n",
    "            print(traceback.format_exc())                               # 打印详细的异常信息  \n",
    "            continue                                                   # 跳过当前循环  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "10c473a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing page 1 of 1\n",
      "geting more info for 1 of 7\n",
      "geting more info for 2 of 7\n",
      "geting more info for 3 of 7\n",
      "geting more info for 4 of 7\n",
      "geting more info for 5 of 7\n",
      "geting more info for 6 of 7\n",
      "geting more info for 7 of 7\n"
     ]
    }
   ],
   "source": [
    "keyword = \"开发\"                                    # 职位的关键字，寻找包含该关键字的职位的具体信息  \n",
    "save_path = \"./html_source\"                         # 设定职位详情保存的路径  \n",
    "  \n",
    "if not os.path.exists(\"./html_source\"):            # 如果保存职位详情的文件夹不存在  \n",
    "    os.mkdir(\"./html_source\")                       # 则创建该文件夹  \n",
    "  \n",
    "total_page_number = 1                             # 爬取第一个列表页  \n",
    "ret_list = parse_html(keyword,total_page_number)    # 批量解析列表页  \n",
    "if ret_list != None:                               # 存放职位链接的列表不为空  \n",
    "    save_html(save_path,ret_list)                   # 批量保存职位链接的网页源码  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd088a51",
   "metadata": {},
   "source": [
    "对上述爬取的网址进行解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e149c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_position_detail(save_path):    \n",
    "    \"\"\"    \n",
    "    利用解析到的职位的url获取职位和公司的详细描述，结果添加到每一职位的dict中    \n",
    "    \"\"\"    \n",
    "    data_list = []                        # 用于存放每个职位的信息  \n",
    "    cnt = 0    \n",
    "    file_list = os.listdir(save_path)    # 所有网页源码的txt文件名  \n",
    "    for file_name in file_list:         #　遍历所有的源代码txt文件  \n",
    "        tmp_dic = {}                     # 用于存放某个职位的具体信息  \n",
    "        print(file_name)    \n",
    "        cnt += 1    \n",
    "        print (\"geting more info for %d of %d\"%(cnt, len(file_list))) # 打印息息解析的进度  \n",
    "        try:  \n",
    "            html_path = os.path.join(save_path,file_name) # 生成完整路径  \n",
    "            html_source = open(html_path).read()          # 读取网页源码  \n",
    "            # print(html_source)    \n",
    "              \n",
    "            soup = BeautifulSoup(html_source, 'html.parser')  \n",
    "              \n",
    "            position_info = soup.find(\"div\", class_=\"cn\")                                              # 职位信息  \n",
    "            title = position_info.h1[\"title\"]                                                          # 职位名称  \n",
    "            salary = position_info.strong.get_text().strip()                                           # 职位薪资  \n",
    "            company_name = position_info.find(\"p\",class_=\"cname\").a['title']                           # 公司名称  \n",
    "              \n",
    "            more_info = position_info.find(\"p\", class_=\"msg ltype\")['title'].replace(\"\\xa0\\xa0\",\"\")  \n",
    "            city = more_info.split(\"|\")[0]                                                             # 工作地点  \n",
    "            experience = more_info.split(\"|\")[1]                                                       # 工作年限  \n",
    "            recruiting_number = more_info.split(\"|\")[2]                                                # 招聘人数  \n",
    "            release_time = more_info.split(\"|\")[3]                                                     # 发布日期  \n",
    "              \n",
    "            company_info = soup.find(\"div\", class_=\"com_tag\")  \n",
    "            company_nature = company_info.findAll(\"p\")[0][\"title\"]                                     # 公司性质  \n",
    "            company_size = company_info.findAll(\"p\")[1][\"title\"]                                       # 公司规模  \n",
    "            industry = company_info.findAll(\"p\")[2][\"title\"]                                           # 行业领域  \n",
    "              \n",
    "            descriptions = soup.find(\"div\",class_=\"bmsg job_msg inbox\").get_text()  \n",
    "            descriptions = re.findall(\"\\[(.*)\\]\",descriptions)[0]                                      # 职位信息  \n",
    "            duty = soup.findAll(\"p\", class_=\"fp\")[0].a.get_text().strip()                              # 职能类别  \n",
    "            address = soup.findAll(\"p\", class_=\"fp\")[1].get_text().strip().replace('上班地址：','')    # 具体地址  \n",
    "              \n",
    "              \n",
    "            tmp_dic['job_name'] = title                                                                # 职位名称  \n",
    "            tmp_dic['provide_salary'] = salary                                                         # 职位薪资  \n",
    "            tmp_dic['company_name'] = company_name                                                     # 公司名称  \n",
    "            tmp_dic['city'] = city                                                                     # 工作地点  \n",
    "            tmp_dic['experience'] = experience                                                         # 工作年限  \n",
    "            tmp_dic['recruiting_number'] = recruiting_number                                           # 招聘人数  \n",
    "            tmp_dic['release_time'] = release_time                                                     # 发布时间  \n",
    "  \n",
    "            tmp_dic['job_info'] = descriptions                                                         # 职位信息  \n",
    "            tmp_dic['job_category'] = duty                                                             # 职能类别  \n",
    "            tmp_dic['company_nature'] = company_nature                                                 # 公司性质    \n",
    "            tmp_dic['company_size'] = company_size                                                     # 公司规模  \n",
    "            tmp_dic['industry'] = industry                                                             # 行业领域  \n",
    "            tmp_dic['address'] = address                                                               # 具体地址  \n",
    "              \n",
    "            data_list.append(tmp_dic)    \n",
    "        except:    \n",
    "            print(traceback.format_exc()) # 打印异常的详细信息  \n",
    "            continue                     # 跳过当前循环  \n",
    "    return data_list  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e85a6666",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(data_list, output_file):  \n",
    "    \"\"\"  \n",
    "    将爬取结果保存到csv文件中  \n",
    "    \"\"\"  \n",
    "    print(\"Save result to %s\"%output_file)  \n",
    "    df = pd.DataFrame(data_list)  \n",
    "    df.to_csv(output_file,index=False,sep=',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dd42b07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.txt\n",
      "geting more info for 1 of 7\n",
      "22.txt\n",
      "geting more info for 2 of 7\n",
      "24.txt\n",
      "geting more info for 3 of 7\n",
      "25.txt\n",
      "geting more info for 4 of 7\n",
      "3.txt\n",
      "geting more info for 5 of 7\n",
      "42.txt\n",
      "geting more info for 6 of 7\n",
      "5.txt\n",
      "geting more info for 7 of 7\n",
      "Save result to output.csv\n"
     ]
    }
   ],
   "source": [
    "ret_positon = get_position_detail(\"./html_source\")  \n",
    "write_to_csv(ret_positon,\"output.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "90228120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>provide_salary</th>\n",
       "      <th>company_name</th>\n",
       "      <th>city</th>\n",
       "      <th>experience</th>\n",
       "      <th>recruiting_number</th>\n",
       "      <th>release_time</th>\n",
       "      <th>job_info</th>\n",
       "      <th>job_category</th>\n",
       "      <th>company_nature</th>\n",
       "      <th>company_size</th>\n",
       "      <th>industry</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>job_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>WEB前端开发</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>北京睿至大数据有限公司</td>\n",
       "      <td>北京</td>\n",
       "      <td>5-10年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/10/16 10:29发布</td>\n",
       "      <td>'工作职责：', '1. 负责公司产品或客户项目的 Web 前端开发；', '2. 负责数据...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>民营</td>\n",
       "      <td>100-499人</td>\n",
       "      <td>IT服务(系统/数据/维护)</td>\n",
       "      <td>北京市朝阳区望京东园四区绿地中心（中国锦）11号楼26层</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEB前端开发</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>上海冰煜信息技术工程有限公司</td>\n",
       "      <td>上海-青浦区</td>\n",
       "      <td>3-5年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/10/17 8:49发布</td>\n",
       "      <td>'岗位职责：', '大型物流企业大屏可视化项目开发；', '任职要求：', '1、本科及以上...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>民营</td>\n",
       "      <td>100-499人</td>\n",
       "      <td>计算机软件</td>\n",
       "      <td>上海市静安区新闸路831号21楼F座</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>数据库开发工程师</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>国创恒星(合肥)软件技术有限公司</td>\n",
       "      <td>上海-浦东新区</td>\n",
       "      <td>1-3年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/9/25 11:00发布</td>\n",
       "      <td>'岗位职责：', '1、负责参与SQL存储过程的编写等数据库应用开发；', '2、对数据库进...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>股份制企业</td>\n",
       "      <td>500-999人</td>\n",
       "      <td>计算机软件</td>\n",
       "      <td>合肥市高新区长江西路669号软件园软件园2号楼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>数据库开发工程师</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>融慧金科金融服务外包(北京)有限公司</td>\n",
       "      <td>北京</td>\n",
       "      <td>1-3年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/10/12 11:30发布</td>\n",
       "      <td>'职位亮点：BAT级别的数据量、金融科技应用。', '', '1、数据产品、工具、平台规划和...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>民营</td>\n",
       "      <td>20-99人</td>\n",
       "      <td>互联网/电子商务</td>\n",
       "      <td>北京市朝阳区西大望路3号蓝堡国际中心1座2303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WEB前端开发</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>京东商城</td>\n",
       "      <td>北京-朝阳区</td>\n",
       "      <td>3-5年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/9/25 11:00发布</td>\n",
       "      <td>'【公司和部门介绍】 京东（JD.com）是中国最大的自营式电商企业，2014年5月，京东在...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>外商独资</td>\n",
       "      <td>1000-9999人</td>\n",
       "      <td>互联网/电子商务</td>\n",
       "      <td>北京市朝阳区北辰西路8号北辰世纪中心A座6层</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>数据库开发工程师</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>泰康养老保险股份有限公司</td>\n",
       "      <td>北京-西城区</td>\n",
       "      <td>5-10年</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/10/16 20:48发布</td>\n",
       "      <td>'岗位职责：', '1.负责大数据相关产品的规划与设计，进行需求分析，形成产品设计以及原型构...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>股份制企业</td>\n",
       "      <td>1000-9999人</td>\n",
       "      <td>基金/证券/期货/投资</td>\n",
       "      <td>北京市西城区复兴门内大街156号泰康人寿大厦A座</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>数据库开发工程师</th>\n",
       "      <td>1-1.5万/月</td>\n",
       "      <td>上海如创网络技术有限公司</td>\n",
       "      <td>上海</td>\n",
       "      <td>不限</td>\n",
       "      <td>招1人</td>\n",
       "      <td>2018/4/26 0:00发布</td>\n",
       "      <td>'因公司业务扩张，现特需大量以下岗位人才，欢迎您踊跃加入我们公司，一起努力共同成长，无经验亦...</td>\n",
       "      <td>UI设计师/顾问</td>\n",
       "      <td>民营</td>\n",
       "      <td>100-499人</td>\n",
       "      <td>通信/电信/网络设备</td>\n",
       "      <td>上海市徐汇区漕支东路81号204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         provide_salary         company_name     city experience  \\\n",
       "job_name                                                           \n",
       "WEB前端开发        1-1.5万/月         北京睿至大数据有限公司        北京      5-10年   \n",
       "WEB前端开发        1-1.5万/月      上海冰煜信息技术工程有限公司    上海-青浦区       3-5年   \n",
       "数据库开发工程师       1-1.5万/月    国创恒星(合肥)软件技术有限公司   上海-浦东新区       1-3年   \n",
       "数据库开发工程师       1-1.5万/月  融慧金科金融服务外包(北京)有限公司        北京       1-3年   \n",
       "WEB前端开发        1-1.5万/月                京东商城    北京-朝阳区       3-5年   \n",
       "数据库开发工程师       1-1.5万/月        泰康养老保险股份有限公司    北京-西城区      5-10年   \n",
       "数据库开发工程师       1-1.5万/月        上海如创网络技术有限公司        上海         不限   \n",
       "\n",
       "         recruiting_number        release_time  \\\n",
       "job_name                                         \n",
       "WEB前端开发                招1人  2018/10/16 10:29发布   \n",
       "WEB前端开发                招1人   2018/10/17 8:49发布   \n",
       "数据库开发工程师               招1人   2018/9/25 11:00发布   \n",
       "数据库开发工程师               招1人  2018/10/12 11:30发布   \n",
       "WEB前端开发                招1人   2018/9/25 11:00发布   \n",
       "数据库开发工程师               招1人  2018/10/16 20:48发布   \n",
       "数据库开发工程师               招1人    2018/4/26 0:00发布   \n",
       "\n",
       "                                                   job_info job_category  \\\n",
       "job_name                                                                   \n",
       "WEB前端开发   '工作职责：', '1. 负责公司产品或客户项目的 Web 前端开发；', '2. 负责数据...     UI设计师/顾问   \n",
       "WEB前端开发   '岗位职责：', '大型物流企业大屏可视化项目开发；', '任职要求：', '1、本科及以上...     UI设计师/顾问   \n",
       "数据库开发工程师  '岗位职责：', '1、负责参与SQL存储过程的编写等数据库应用开发；', '2、对数据库进...     UI设计师/顾问   \n",
       "数据库开发工程师  '职位亮点：BAT级别的数据量、金融科技应用。', '', '1、数据产品、工具、平台规划和...     UI设计师/顾问   \n",
       "WEB前端开发   '【公司和部门介绍】 京东（JD.com）是中国最大的自营式电商企业，2014年5月，京东在...     UI设计师/顾问   \n",
       "数据库开发工程师  '岗位职责：', '1.负责大数据相关产品的规划与设计，进行需求分析，形成产品设计以及原型构...     UI设计师/顾问   \n",
       "数据库开发工程师  '因公司业务扩张，现特需大量以下岗位人才，欢迎您踊跃加入我们公司，一起努力共同成长，无经验亦...     UI设计师/顾问   \n",
       "\n",
       "         company_nature company_size        industry  \\\n",
       "job_name                                               \n",
       "WEB前端开发              民营     100-499人  IT服务(系统/数据/维护)   \n",
       "WEB前端开发              民营     100-499人           计算机软件   \n",
       "数据库开发工程师          股份制企业     500-999人           计算机软件   \n",
       "数据库开发工程师             民营       20-99人        互联网/电子商务   \n",
       "WEB前端开发            外商独资   1000-9999人        互联网/电子商务   \n",
       "数据库开发工程师          股份制企业   1000-9999人     基金/证券/期货/投资   \n",
       "数据库开发工程师             民营     100-499人      通信/电信/网络设备   \n",
       "\n",
       "                               address  \n",
       "job_name                                \n",
       "WEB前端开发   北京市朝阳区望京东园四区绿地中心（中国锦）11号楼26层  \n",
       "WEB前端开发             上海市静安区新闸路831号21楼F座  \n",
       "数据库开发工程师       合肥市高新区长江西路669号软件园软件园2号楼  \n",
       "数据库开发工程师      北京市朝阳区西大望路3号蓝堡国际中心1座2303  \n",
       "WEB前端开发         北京市朝阳区北辰西路8号北辰世纪中心A座6层  \n",
       "数据库开发工程师      北京市西城区复兴门内大街156号泰康人寿大厦A座  \n",
       "数据库开发工程师              上海市徐汇区漕支东路81号204  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('output.csv',index_col=0) \n",
    "data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
